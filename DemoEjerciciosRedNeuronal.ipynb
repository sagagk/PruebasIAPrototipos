{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DemoEjerciciosRedNeuronal.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qj6h3zi3FS0o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, BatchNormalization, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten\n",
        "from keras.models import  Model\n",
        "import sklearn.metrics as metrics\n",
        "import pandas as pd\n",
        "from keras.utils import np_utils\n",
        "# from tensorflow import set_random_seed\n",
        "import tensorflow\n",
        "# tensorflow.random.set_seed(x)\n",
        "import random\n",
        "random.seed(0)\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "activity_list = ['01', '02', '03', '04', '05', '06', '07']\n",
        "id_list = range(len(activity_list))\n",
        "activity_id_dict = dict(zip(activity_list, id_list))\n"
      ],
      "metadata": {
        "id": "ERiX9U2wGEue"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path = '/home/mex/data/dc_scaled/0.05_0.05/'\n",
        "# results_file = '/home/mex/results_lopo/cnn_dc_fold.csv'\n",
        "path = '/content/drive/My Drive/data/0.05_0.05/'\n",
        "results_file = 'cnn_dc_fold.csv'"
      ],
      "metadata": {
        "id": "o70h9KARGFvc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames_per_second = 1\n",
        "window = 5\n",
        "increment = 2\n",
        "\n",
        "dc_min_length = frames_per_second*window\n",
        "dc_max_length = 15*window\n",
        "\n",
        "\n",
        "def write_data(file_path, data):\n",
        "    if os.path.isfile(file_path):\n",
        "        f = open(file_path, 'a')\n",
        "        f.write(data + '\\n')\n",
        "    else:\n",
        "        f = open(file_path, 'w')\n",
        "        f.write(data + '\\n')\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def _read(_file):\n",
        "    reader = csv.reader(open(_file, \"r\"), delimiter=\",\")\n",
        "    _data = []\n",
        "    for row in reader:\n",
        "        if len(row[0]) == 19 and '.' not in row[0]:\n",
        "            row[0] = row[0]+'.000000'\n",
        "        temp = [dt.datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S.%f')]\n",
        "        _temp = [float(f) for f in row[1:]]\n",
        "        temp.extend(_temp)\n",
        "        _data.append(temp)\n",
        "    return _data\n",
        "\n",
        "\n",
        "def read():\n",
        "    alldata = {}\n",
        "    subjects = os.listdir(path)\n",
        "    for subject in subjects:\n",
        "        allactivities = {}\n",
        "        subject_path = os.path.join(path, subject)\n",
        "        activities = os.listdir(subject_path)\n",
        "        for activity in activities:\n",
        "            sensor = activity.split('.')[0].replace('_dc', '')\n",
        "            activity_id = sensor.split('_')[0]\n",
        "            _data = _read(os.path.join(subject_path, activity), )\n",
        "            if activity_id in allactivities:\n",
        "                allactivities[activity_id][sensor] = _data\n",
        "            else:\n",
        "                allactivities[activity_id] = {}\n",
        "                allactivities[activity_id][sensor] = _data\n",
        "        alldata[subject] = allactivities\n",
        "    return alldata\n",
        "\n",
        "\n",
        "def find_index(_data, _time_stamp):\n",
        "    return [_index for _index, _item in enumerate(_data) if _item[0] >= _time_stamp][0]\n",
        "\n",
        "\n",
        "def trim(_data):\n",
        "    _length = len(_data)\n",
        "    _inc = _length/(window*frames_per_second)\n",
        "    _new_data = []\n",
        "    for i in range(window*frames_per_second):\n",
        "        _new_data.append(_data[i*_inc])\n",
        "    return _new_data\n",
        "\n",
        "\n",
        "def frame_reduce(_data):\n",
        "    if frames_per_second == 0:\n",
        "        return _data\n",
        "    _features = {}\n",
        "    for subject in _data:\n",
        "        _activities = {}\n",
        "        activities = _data[subject]\n",
        "        for activity in activities:\n",
        "            activity_data = activities[activity]\n",
        "            time_windows = []\n",
        "            for item in activity_data:\n",
        "                time_windows.append(trim(item))\n",
        "            _activities[activity] = time_windows\n",
        "        _features[subject] = _activities\n",
        "    return _features\n",
        "\n",
        "\n",
        "def split_windows(data):\n",
        "    outputs = []\n",
        "    start = data[0][0]\n",
        "    end = data[len(data) - 1][0]\n",
        "    _increment = dt.timedelta(seconds=increment)\n",
        "    _window = dt.timedelta(seconds=window)\n",
        "\n",
        "    frames = [a[1:] for a in data[:]]\n",
        "    frames = np.array(frames)\n",
        "\n",
        "    while start + _window < end:\n",
        "        _end = start + _window\n",
        "        start_index = find_index(data, start)\n",
        "        end_index = find_index(data, _end)\n",
        "        instances = [a[:] for a in frames[start_index:end_index]]\n",
        "        start = start + _increment\n",
        "        outputs.append(instances)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "# single sensor\n",
        "def extract_features(_data):\n",
        "    _features = {}\n",
        "    for subject in _data:\n",
        "        _activities = {}\n",
        "        activities = _data[subject]\n",
        "        for activity in activities:\n",
        "            time_windows = []\n",
        "            activity_id = activity_id_dict.get(activity)\n",
        "            activity_data = activities[activity]\n",
        "            for sensor in activity_data:\n",
        "                time_windows.extend(split_windows(activity_data[sensor]))\n",
        "            _activities[activity_id] = time_windows\n",
        "        _features[subject] = _activities\n",
        "    return _features\n",
        "\n",
        "\n",
        "def train_test_split(user_data, test_ids):\n",
        "    train_data = {key: value for key, value in user_data.items() if key not in test_ids}\n",
        "    test_data = {key: value for key, value in user_data.items() if key in test_ids}\n",
        "    return train_data, test_data\n",
        "\n",
        "\n",
        "def flatten(_data):\n",
        "    flatten_data = []\n",
        "    flatten_labels = []\n",
        "\n",
        "    for subject in _data:\n",
        "        activities = _data[subject]\n",
        "        for activity in activities:\n",
        "            activity_data = activities[activity]\n",
        "            flatten_data.extend(activity_data)\n",
        "            flatten_labels.extend([activity for i in range(len(activity_data))])\n",
        "    return flatten_data, flatten_labels\n",
        "\n",
        "\n",
        "def pad(data, length):\n",
        "    pad_length = []\n",
        "    if length % 2 == 0:\n",
        "        pad_length = [int(length / 2), int(length / 2)]\n",
        "    else:\n",
        "        pad_length = [int(length / 2) + 1, int(length / 2)]\n",
        "    new_data = []\n",
        "    for index in range(pad_length[0]):\n",
        "        new_data.append(data[0])\n",
        "    new_data.extend(data)\n",
        "    for index in range(pad_length[1]):\n",
        "        new_data.append(data[len(data) - 1])\n",
        "    return new_data\n",
        "\n",
        "\n",
        "def reduce(data, length):\n",
        "    red_length = []\n",
        "    if length % 2 == 0:\n",
        "        red_length = [int(length / 2), int(length / 2)]\n",
        "    else:\n",
        "        red_length = [int(length / 2) + 1, int(length / 2)]\n",
        "    new_data = data[red_length[0]:len(data) - red_length[1]]\n",
        "    return new_data\n",
        "\n",
        "\n",
        "def pad_features(_features):\n",
        "    new_features = {}\n",
        "    for subject in _features:\n",
        "        new_activities = {}\n",
        "        activities = _features[subject]\n",
        "        for act in activities:\n",
        "            items = activities[act]\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                _len = len(item)\n",
        "                if _len < dc_min_length:\n",
        "                    continue\n",
        "                elif _len > dc_max_length:\n",
        "                    item = reduce(item, _len - dc_max_length)\n",
        "                    new_items.append(item)\n",
        "                elif _len < dc_max_length:\n",
        "                    item = pad(item, dc_max_length - _len)\n",
        "                    new_items.append(item)\n",
        "                elif _len == dc_max_length:\n",
        "                    new_items.append(item)\n",
        "            new_activities[act] = new_items\n",
        "        new_features[subject] = new_activities\n",
        "    return new_features\n",
        "\n",
        "\n",
        "def build_2D_model():\n",
        "    _input = Input(shape=(12, 16 * window * frames_per_second, 1))\n",
        "    x = Conv2D(32, kernel_size=(3,3), activation='relu')(_input)\n",
        "    x = MaxPooling2D(pool_size=2, data_format='channels_last')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64, kernel_size=(3,3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=2, data_format='channels_last')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(600, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(100, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(len(activity_list), activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=_input, outputs=x)\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "def _run_(_train_features, _train_labels, _test_features, _test_labels):\n",
        "    _train_features = np.array(_train_features)\n",
        "    print(_train_features.shape)\n",
        "\n",
        "    _train_features = np.reshape(_train_features, (_train_features.shape[0], _train_features.shape[1], 12, 16))\n",
        "    _train_features = np.swapaxes(_train_features, 1, 2)\n",
        "    _train_features = np.swapaxes(_train_features, 2, 3)\n",
        "    _train_features = np.reshape(_train_features, (_train_features.shape[0], _train_features.shape[1],\n",
        "                                                   _train_features.shape[2] * _train_features.shape[3]))\n",
        "    _train_features = np.expand_dims(_train_features, 4)\n",
        "    print(_train_features.shape)\n",
        "\n",
        "    _test_features = np.array(_test_features)\n",
        "    print(_test_features.shape)\n",
        "    _test_features = np.reshape(_test_features, (_test_features.shape[0], _test_features.shape[1], 12, 16))\n",
        "    _test_features = np.swapaxes(_test_features, 1, 2)\n",
        "    _test_features = np.swapaxes(_test_features, 2, 3)\n",
        "    _test_features = np.reshape(_test_features, (_test_features.shape[0], _test_features.shape[1],\n",
        "                                                 _test_features.shape[2] * _test_features.shape[3]))\n",
        "    _test_features = np.expand_dims(_test_features, 4)\n",
        "    print(_test_features.shape)\n",
        "\n",
        "    model = build_2D_model()\n",
        "    model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(_train_features, _train_labels, verbose=0, batch_size=32, epochs=50, shuffle=True)\n",
        "    _predict_labels = model.predict(_test_features, batch_size=64, verbose=0)\n",
        "    f_score = metrics.f1_score(_test_labels.argmax(axis=1), _predict_labels.argmax(axis=1), average='macro')\n",
        "    accuracy = metrics.accuracy_score(_test_labels.argmax(axis=1), _predict_labels.argmax(axis=1))\n",
        "    results = 'dc' + ',' + '2D' + ',' + str(accuracy)+',' + str(f_score)\n",
        "    print(results)\n",
        "    write_data(results_file, str(results))\n",
        "\n",
        "    _test_labels = pd.Series(_test_labels.argmax(axis=1), name='Actual')\n",
        "    _predict_labels = pd.Series(_predict_labels.argmax(axis=1), name='Predicted')\n",
        "    df_confusion = pd.crosstab(_test_labels, _predict_labels)\n",
        "    print(df_confusion)\n",
        "    write_data(results_file, str(df_confusion))\n",
        "\n",
        "\n",
        "def holdout_train_test_split(_data, test_ids):\n",
        "    _train = {key: value for key, value in _data.items() if key not in test_ids}\n",
        "    _test = {key: value for key, value in _data.items() if key in test_ids}\n",
        "    return _train, _test\n",
        "\n",
        "\n",
        "def get_hold_out_users(users):\n",
        "    indices = np.random.choice(len(users), int(len(users) / 3), False)\n",
        "    test_users = [u for indd, u in enumerate(users) if indd in indices]\n",
        "    return test_users"
      ],
      "metadata": {
        "id": "BRjjjtxAGbT0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "EbgFU1SFH961",
        "outputId": "52f8c1ef-10d9-495c-b43b-af32859dee53"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 128\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = read()\n",
        "all_features = extract_features(all_data)\n",
        "all_data = None\n",
        "all_features = pad_features(all_features)\n",
        "all_features = frame_reduce(all_features)\n",
        "\n",
        "test_user_ids = get_hold_out_users(list(all_features.keys()))\n",
        "train_features, test_features = holdout_train_test_split(all_features, test_user_ids)\n",
        "#train_features, test_features = train_test_split(all_features, [i])\n",
        "\n",
        "train_features, train_labels = flatten(train_features)\n",
        "test_features, test_labels = flatten(test_features)\n",
        "\n",
        "train_labels = np_utils.to_categorical(train_labels, len(activity_list))\n",
        "test_labels = np_utils.to_categorical(test_labels, len(activity_list))\n",
        "\n",
        "_run_(train_features, train_labels, test_features, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "RKACPhU3GdLr",
        "outputId": "51b732a5-0372-4719-91aa-6442d77c6f33"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b9c09f1e6958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-3355ee5d360a>\u001b[0m in \u001b[0;36mread\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0malldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0msubjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubjects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mallactivities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/0.05_0.05/'"
          ]
        }
      ]
    }
  ]
}